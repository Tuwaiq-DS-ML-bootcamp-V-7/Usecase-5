{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e69c4ba1-85f3-4a02-ae37-c64347a01803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "## This statement allows the visuals to render within your Jupyter Notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3548ef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'{sys.executable}' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# -m running module as a script and -U upgrading \n",
    "!{sys.executable} -m pip install -U ydata-profiling[notebook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "678e7342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'{sys.executable}' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install -U sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b92e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c5ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395e8bf-b530-4720-8d0f-26811e927d6e",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "We can now load the dataset into pandas using the read_csv() function. This converts the CSV file into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf4030b0-aa7f-4bd7-8e9c-c1b7f4d202d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the csv file and convert to a Pandas dataframe\n",
    "df = pd.read_csv('Jadarat_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ed983-0f72-43eb-8168-6fafd48c562e",
   "metadata": {},
   "source": [
    "### Viewing the dataframe\n",
    "We can get a quick sense of the size of our dataset by using the shape method. This returns a tuple with the number of rows and columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfde0b69-613c-4dd1-8ebf-e4d2e570e653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1470, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have 1470 rows and 18 columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b5bc4-db37-4432-b666-0714afd0c4ca",
   "metadata": {},
   "source": [
    "## 1. Data Profiling:\n",
    "Data profiling is a comprehensive process of examining the data available in an existing dataset and collecting statistics and information about that data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "672f2081-5a2c-4908-8cbd-29519fb3cac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_date</th>\n",
       "      <th>job_desc</th>\n",
       "      <th>job_tasks</th>\n",
       "      <th>comp_name</th>\n",
       "      <th>comp_no</th>\n",
       "      <th>comp_type</th>\n",
       "      <th>comp_size</th>\n",
       "      <th>eco_activity</th>\n",
       "      <th>qualif</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>benefits</th>\n",
       "      <th>contract</th>\n",
       "      <th>positions</th>\n",
       "      <th>job_post_id</th>\n",
       "      <th>exper</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>محاسب</td>\n",
       "      <td>27/05/1444</td>\n",
       "      <td>['إعداد وتنظيم مستندات الصرف ومتابعة تحصيل الإ...</td>\n",
       "      <td>['   تدقيق المطالبات المالية والتأكد من اكتمال...</td>\n",
       "      <td>شركة مقر العالم للسفريات</td>\n",
       "      <td>1-317262</td>\n",
       "      <td>خاص</td>\n",
       "      <td>متوسطة فئة أ</td>\n",
       "      <td>أنشطة وكالات السياحة والسفر</td>\n",
       "      <td>['Language data', 'اللغة الانجليزية', 'محترف']</td>\n",
       "      <td>الرياض</td>\n",
       "      <td>AR RIYADH...</td>\n",
       "      <td>['Salary', '5000.0']</td>\n",
       "      <td>دوام كامل</td>\n",
       "      <td>0 / 1</td>\n",
       "      <td>20202026350419</td>\n",
       "      <td>0 Years</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>بائع</td>\n",
       "      <td>27/05/1444</td>\n",
       "      <td>['بيع مجموعة من السلع والخدمات للعملاء، وتوفير...</td>\n",
       "      <td>['   بيع مجموعة من السلع والخدمات للعملاء.', '...</td>\n",
       "      <td>شركة عالم الكهرباء للمقاولات</td>\n",
       "      <td>4-1324428</td>\n",
       "      <td>خاص</td>\n",
       "      <td>متوسطة فئة ب</td>\n",
       "      <td>تركيب انظمة التبريد وتكييف الهواء وصيانتها واص...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>المنطقة الشرقية</td>\n",
       "      <td>AD DAMMAM...</td>\n",
       "      <td>['Salary', '5000.0']</td>\n",
       "      <td>دوام كامل</td>\n",
       "      <td>0 / 3</td>\n",
       "      <td>20202026350389</td>\n",
       "      <td>0 Years</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>أخصائي عمليات موارد بشرية</td>\n",
       "      <td>27/05/1444</td>\n",
       "      <td>['تنفيذ الإجراءات والأنظمة والنماذج الخاصة بمر...</td>\n",
       "      <td>['   تنفيذ الإجراءات والأنظمة والنماذج الخاصة ...</td>\n",
       "      <td>شركه دار السلام</td>\n",
       "      <td>1-155294</td>\n",
       "      <td>خاص</td>\n",
       "      <td>متوسطة فئة أ</td>\n",
       "      <td>ترميمات المباني السكنية والغير سكنية</td>\n",
       "      <td>['Language data', 'الانجليزيه', 'محترف']</td>\n",
       "      <td>الرياض</td>\n",
       "      <td>ATH THUMA...</td>\n",
       "      <td>['Salary', '4000.0']</td>\n",
       "      <td>دوام كامل</td>\n",
       "      <td>0 / 2</td>\n",
       "      <td>20202026350347</td>\n",
       "      <td>2 Years</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ميكانيكي سيارات</td>\n",
       "      <td>27/05/1444</td>\n",
       "      <td>['تشخيص أعطال السيارات وإصلاحها وتنفيذ برامج ا...</td>\n",
       "      <td>['   فحص أداء المعدّات الكهربائية والميكانيكية...</td>\n",
       "      <td>مؤسسة لمكو لغيار الزيوت</td>\n",
       "      <td>8-1925495</td>\n",
       "      <td>خاص</td>\n",
       "      <td>صغيرة فئة ب</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Skill data', 'صيانة السيارات وتقييم الاعطال'...</td>\n",
       "      <td>المنطقة الشرقية</td>\n",
       "      <td>AL HUFUF...</td>\n",
       "      <td>['Salary', '5000.0']</td>\n",
       "      <td>دوام كامل</td>\n",
       "      <td>0 / 10</td>\n",
       "      <td>20202026350219</td>\n",
       "      <td>0 Years</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>محاسب</td>\n",
       "      <td>27/05/1444</td>\n",
       "      <td>['إعداد وتنظيم مستندات الصرف ومتابعة تحصيل الإ...</td>\n",
       "      <td>['   تدقيق المطالبات المالية والتأكد من اكتمال...</td>\n",
       "      <td>مؤسسة فكرة اليمامة للمقاولات</td>\n",
       "      <td>1-2356639</td>\n",
       "      <td>خاص</td>\n",
       "      <td>كبيرة</td>\n",
       "      <td>الإنشاءات العامة للمباني السكنية</td>\n",
       "      <td>['Skill data', 'تحمل ضغط العمل', 'محترف', 'Lan...</td>\n",
       "      <td>الرياض</td>\n",
       "      <td>AR RIYADH...</td>\n",
       "      <td>['Salary', '5000.0']</td>\n",
       "      <td>دوام كامل</td>\n",
       "      <td>0 / 1</td>\n",
       "      <td>20202026350043</td>\n",
       "      <td>0 Years</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   job_title    job_date  \\\n",
       "0                      محاسب  27/05/1444   \n",
       "1                       بائع  27/05/1444   \n",
       "2  أخصائي عمليات موارد بشرية  27/05/1444   \n",
       "3            ميكانيكي سيارات  27/05/1444   \n",
       "4                      محاسب  27/05/1444   \n",
       "\n",
       "                                            job_desc  \\\n",
       "0  ['إعداد وتنظيم مستندات الصرف ومتابعة تحصيل الإ...   \n",
       "1  ['بيع مجموعة من السلع والخدمات للعملاء، وتوفير...   \n",
       "2  ['تنفيذ الإجراءات والأنظمة والنماذج الخاصة بمر...   \n",
       "3  ['تشخيص أعطال السيارات وإصلاحها وتنفيذ برامج ا...   \n",
       "4  ['إعداد وتنظيم مستندات الصرف ومتابعة تحصيل الإ...   \n",
       "\n",
       "                                           job_tasks  \\\n",
       "0  ['   تدقيق المطالبات المالية والتأكد من اكتمال...   \n",
       "1  ['   بيع مجموعة من السلع والخدمات للعملاء.', '...   \n",
       "2  ['   تنفيذ الإجراءات والأنظمة والنماذج الخاصة ...   \n",
       "3  ['   فحص أداء المعدّات الكهربائية والميكانيكية...   \n",
       "4  ['   تدقيق المطالبات المالية والتأكد من اكتمال...   \n",
       "\n",
       "                      comp_name    comp_no comp_type     comp_size  \\\n",
       "0      شركة مقر العالم للسفريات   1-317262       خاص  متوسطة فئة أ   \n",
       "1  شركة عالم الكهرباء للمقاولات  4-1324428       خاص  متوسطة فئة ب   \n",
       "2               شركه دار السلام   1-155294       خاص  متوسطة فئة أ   \n",
       "3       مؤسسة لمكو لغيار الزيوت  8-1925495       خاص   صغيرة فئة ب   \n",
       "4  مؤسسة فكرة اليمامة للمقاولات  1-2356639       خاص         كبيرة   \n",
       "\n",
       "                                        eco_activity  \\\n",
       "0                        أنشطة وكالات السياحة والسفر   \n",
       "1  تركيب انظمة التبريد وتكييف الهواء وصيانتها واص...   \n",
       "2               ترميمات المباني السكنية والغير سكنية   \n",
       "3                                                NaN   \n",
       "4                   الإنشاءات العامة للمباني السكنية   \n",
       "\n",
       "                                              qualif           region  \\\n",
       "0     ['Language data', 'اللغة الانجليزية', 'محترف']           الرياض   \n",
       "1                                                NaN  المنطقة الشرقية   \n",
       "2           ['Language data', 'الانجليزيه', 'محترف']           الرياض   \n",
       "3  ['Skill data', 'صيانة السيارات وتقييم الاعطال'...  المنطقة الشرقية   \n",
       "4  ['Skill data', 'تحمل ضغط العمل', 'محترف', 'Lan...           الرياض   \n",
       "\n",
       "           city              benefits   contract positions     job_post_id  \\\n",
       "0  AR RIYADH...  ['Salary', '5000.0']  دوام كامل     0 / 1  20202026350419   \n",
       "1  AD DAMMAM...  ['Salary', '5000.0']  دوام كامل     0 / 3  20202026350389   \n",
       "2  ATH THUMA...  ['Salary', '4000.0']  دوام كامل     0 / 2  20202026350347   \n",
       "3   AL HUFUF...  ['Salary', '5000.0']  دوام كامل    0 / 10  20202026350219   \n",
       "4  AR RIYADH...  ['Salary', '5000.0']  دوام كامل     0 / 1  20202026350043   \n",
       "\n",
       "     exper gender  \n",
       "0  0 Years   both  \n",
       "1  0 Years   both  \n",
       "2  2 Years   both  \n",
       "3  0 Years      M  \n",
       "4  0 Years   both  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ad3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "834929aa-fd6a-4ec5-84d4-77c4b3c1a506",
   "metadata": {},
   "source": [
    "The process of profiling differs slightly for categorical and numerical variables due to their inherent differences.\n",
    "\n",
    "**The two main types of data are:**\n",
    "- Quantitative (numerical) data\n",
    "- Qualitative (categorical) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9954f5-6fbc-45b4-ad86-3e21b6e0ca2d",
   "metadata": {},
   "source": [
    "### Data Quality Checks\n",
    "Data quality checks involve the process of ensuring that the data is accurate, complete, consistent, relevant, and reliable. \n",
    "\n",
    "\n",
    "**Here are typical steps involved in checking data quality:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4818c876-173d-4e56-9e7d-b4334d2def25",
   "metadata": {},
   "source": [
    "#### 1. Reliability:\n",
    "Evaluate the data's source and collection process to determine its trustworthiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9034ae7b-dc1d-4cba-8f9e-bb499d021cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes data is reliable since it was collected from the data source Jadarat and mentioned it in Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce0cc59-8c92-4acc-8d07-c40764e1a86b",
   "metadata": {},
   "source": [
    "#### 2. Timeliness: \n",
    "Ensure the data is up-to-date and reflective of the current situation or the period of interest for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b78ae35-7226-4cb6-b8b2-a46c2ed17cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is not considered up to date since its last update was in 2023, and the last job dates posted in Jadarat were in 2024\n",
    "# But since our objective questions didn't focused on the last updated year, and last updated 2023 seemms close to last updated in Jadarat\n",
    "# I will continuou working in this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fab0fe2-6624-4615-b9d8-3c3669056bf8",
   "metadata": {},
   "source": [
    "#### 3. Consistency: \n",
    "\n",
    "Confirm that the data is consistent within the dataset and across multiple data sources. For example, the same data point should not have different values in different places.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fecc573-959f-4800-8ddd-a67985c68b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have here one dataset file it's consistent, and for the data source "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec3d183-bba0-4b12-b963-487daab1e876",
   "metadata": {},
   "source": [
    "#### 4. Relevance: \n",
    "Assess whether the data is appropriate and applicable for the intended analysis. Data that is not relevant can skew results and lead to incorrect conclusions.\n",
    "\n",
    "**Key considerations for relevance include:**\n",
    "\n",
    "> 1. Sample Appropriateness: Confirm that your data sample aligns with your analysis objectives. For instance, utilizing data from the Northern region will not yield accurate insights for the Western region of the Kingdom.\n",
    ">\n",
    "> 2. Variable Selection: Any column will not be relevant for our analysis, we can get rid of these using the drop() method. We will set the “axis” argument to 1 since we’re dealing with columns, and set the “inplace” argument to True to make the change permanent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50a934b7-a0cd-443f-8a27-f10e9a6e8647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['job_title', 'job_date', 'job_desc', 'job_tasks', 'comp_name',\n",
       "       'comp_no', 'comp_type', 'comp_size', 'eco_activity', 'qualif', 'region',\n",
       "       'city', 'benefits', 'contract', 'positions', 'job_post_id', 'exper',\n",
       "       'gender'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yes, the dataset sample is relevant to the objective questions and for the columns:\n",
    "# Job date is not useful for us and we won't get any insight from it because the end user looking for an analysis\n",
    "# doesn't want to know about which data posted but more about which jobs and posistions related to other factors\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e3a2a-bbb7-4d06-8220-c93277b70146",
   "metadata": {},
   "source": [
    "#### 5. Uniqueness: \n",
    "Check for and remove duplicate records to prevent skewed analysis results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3c7c151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a509a7aa-58f4-4d39-8eb8-e8298a21f2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_date</th>\n",
       "      <th>job_desc</th>\n",
       "      <th>job_tasks</th>\n",
       "      <th>comp_name</th>\n",
       "      <th>comp_no</th>\n",
       "      <th>comp_type</th>\n",
       "      <th>comp_size</th>\n",
       "      <th>eco_activity</th>\n",
       "      <th>qualif</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>benefits</th>\n",
       "      <th>contract</th>\n",
       "      <th>positions</th>\n",
       "      <th>job_post_id</th>\n",
       "      <th>exper</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>صيدلي</td>\n",
       "      <td>18/06/1444</td>\n",
       "      <td>['دراسة التركيبات الدوائية للعقاقير والعلاجات ...</td>\n",
       "      <td>['   دراسة مواصفات وتركيبات الادوية والعقاقير ...</td>\n",
       "      <td>شركة تقنية الدواجن والألبان للتجارة</td>\n",
       "      <td>1-2415250</td>\n",
       "      <td>خاص</td>\n",
       "      <td>صغيرة فئة ب</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Professional certificate \\\\ License data', '...</td>\n",
       "      <td>الرياض</td>\n",
       "      <td>AR RIYADH...</td>\n",
       "      <td>['Salary', '7000.0']</td>\n",
       "      <td>دوام كامل</td>\n",
       "      <td>0 / 1</td>\n",
       "      <td>20202026399061</td>\n",
       "      <td>0 Years</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     job_title    job_date                                           job_desc  \\\n",
       "1469     صيدلي  18/06/1444  ['دراسة التركيبات الدوائية للعقاقير والعلاجات ...   \n",
       "\n",
       "                                              job_tasks  \\\n",
       "1469  ['   دراسة مواصفات وتركيبات الادوية والعقاقير ...   \n",
       "\n",
       "                                comp_name    comp_no comp_type    comp_size  \\\n",
       "1469  شركة تقنية الدواجن والألبان للتجارة  1-2415250       خاص  صغيرة فئة ب   \n",
       "\n",
       "     eco_activity                                             qualif  region  \\\n",
       "1469          NaN  ['Professional certificate \\\\ License data', '...  الرياض   \n",
       "\n",
       "              city              benefits   contract positions     job_post_id  \\\n",
       "1469  AR RIYADH...  ['Salary', '7000.0']  دوام كامل     0 / 1  20202026399061   \n",
       "\n",
       "        exper gender  \n",
       "1469  0 Years   both  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()]\n",
    "# since this job position is duplicated from the same company i will drop it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b271b9b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'duplicates'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_57984\\1503988213.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'duplicates'"
     ]
    }
   ],
   "source": [
    "df.duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c62559-3b48-48cf-a4e9-857e2e0ff416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to delete duplicates columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7724ac2-4ea2-4cf4-a9e0-e64368f87b92",
   "metadata": {},
   "source": [
    "#### 6. Completeness: \n",
    "Ensure that no critical data is missing. This might mean checking for null values or required fields that are empty.\n",
    "\n",
    "We will start by checking the dataset for missing or null values. For this, we can use the isna() method which returns a dataframe of boolean values indicating if a field is null or not. To group all missing values by column, we can include the sum() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a4de54-a344-4b94-9908-9528c15c13f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display number missing values per column\n",
    "df.isnull().sum()\n",
    "# We can see that column comp_size has 14 null calues, eco_activity has 83, and qualif has 667 which is huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf32dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nulls(x):\n",
    "    return x in ['/', ' ', '-']\n",
    "    \n",
    "null_values = df.applymap(check_nulls)\n",
    "\n",
    "rows_with_nulls = df[null_values.any(axis=1)]\n",
    "\n",
    "rows_with_nulls # there is no null values as space or dash or slash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec31c4-0904-4b73-8cee-b7bc14ab1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to clean them "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40de63c-5a1d-49ed-a87e-c5229ee08bbe",
   "metadata": {},
   "source": [
    "#### 7. Check Accuracy:\n",
    "\n",
    "Verify that the data is correct and precise. This could involve comparing data samples with known sources or using validation rules.\n",
    "\n",
    "**The process includes:**\n",
    "1. Validating the appropriateness of data types for the dataset.\n",
    "2. Identifying outliers  using established validation  rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be5334-ae10-4abd-8097-3259fe5e72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns types \n",
    "# it shows that positions column needs to change to an int, because 0/number doesn't give any meaning and it might give confusion\n",
    "# and also the benifits column which is the salary\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee790ba-dcc0-45f1-b6f8-0133e913e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to clean them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1086ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking EDA advanced analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd47b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this report gives a comprehensive analysis of the dataset but in my opinion it's not showing the whole analysis of the important EDA steps\n",
    "# and because the values are in arabic language which affects the analysis \n",
    "ProfileReport(df, title='Employement of Jadarat Report in KSA', explorative=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca9d42-add6-45db-92cf-1f6dae5f277b",
   "metadata": {},
   "source": [
    "**What is an Outlier?** \n",
    "Outlier is an row/observation that appears far away and diverges from an overall pattern in a sample.\n",
    "\n",
    "**What are the types of Outliers?**\n",
    "1. Univariate: These outliers can be found when we look at distribution of a single variable\n",
    "2. Multivariate: are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions. example (hight=100, weight=100) for a person\n",
    "\n",
    "**What causes Outliers?**\n",
    "Whenever we come across outliers, the ideal way to tackle them is to find out the reason of having these outliers. The method to deal with them would then depend on the reason of their occurrence.\n",
    "\n",
    "Let’s understand various types of outliers:\n",
    "\n",
    "1. Data Entry Errors:- Human errors such as errors caused during data collection, recording, or entry can cause outliers in data.\n",
    "2. Measurement Error: It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty.\n",
    "3. Data Processing Error: Whenever we perform data mining, we extract data from multiple sources. It is possible that some manipulation or extraction errors may lead to outliers in the dataset.\n",
    "4. Sampling error: For instance, we have to measure the height of athletes. By mistake, we include a few basketball players in the sample. This inclusion is likely to cause outliers in the dataset.\n",
    "5. Natural Outlier: When an outlier is not artificial (due to error), it is a natural outlier. For instance: In my last assignment with one of the renowned insurance company, I noticed that the performance of top 50 financial advisors was far higher than rest of the population. Surprisingly, it was not due to any error. Hence, whenever we perform any data mining activity with advisors, we used to treat this segment separately.\n",
    "\n",
    "\n",
    "**What is the impact of Outliers on a dataset?**\n",
    "\n",
    "\n",
    "![image.png](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Outlier_31.png)\n",
    "\n",
    "\n",
    "\n",
    "**How to detect Outliers?**\n",
    "\n",
    "1. Most commonly used method to detect outliers is visualization (Univariate Graphical Analysis).\n",
    "\n",
    "We use 3 common visualization methods:\n",
    ">- Box-plot: A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of the box to show the range of the data. Outlier points are those past the end of the whiskers. Box plots show robust measures of location and spread as well as providing information about symmetry and outliers.\n",
    ">\n",
    ">  \n",
    ">![image.png](https://miro.medium.com/v2/resize:fit:698/format:webp/1*VK5iHA2AB28HSZwWwUbNYg.png)\n",
    ">\n",
    ">\n",
    ">- Histogram\n",
    ">- Scatter Plot: A scatter plot is a mathematical diagram using Cartesian coordinates to display values for two variables for a set of data. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. The points that are far from the population can be termed as an outlier.\n",
    ">\n",
    ">  \n",
    ">![image.png](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*Ov6aH-8yIwNoUxtMFwgx4g.png)\n",
    ">\n",
    ">\n",
    "\n",
    "2. Using statistical method (Univariate Non-Graphical analysis):\n",
    ">- Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR\n",
    " \n",
    "![image.png](https://www.whatissixsigma.net/wp-content/uploads/2015/07/Box-Plot-Diagram-to-identify-Outliers-figure-1.png)\n",
    "\n",
    ">- Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier\n",
    ">- Data points, three or more standard deviation away from mean are considered outlier: The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured. While calculating the Z-score we re-scale and center the data and look for data points that are too far from zero. These data points which are way too far from zero will be treated as the outliers. In most of the cases, a threshold of 3 or -3 is used i.e if the Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outliers.\n",
    "> - Outlier detection is merely a special case of the examination of data for influential data points and it also depends on the business understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea6194-cc01-45d8-be38-c4543eb1714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to univariate graphical analysis\n",
    "# go to lesson : data visualisation 1 - chart type section\n",
    "# then go to univariate graphical analysis\n",
    "# detect outliers using graphs varbaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecf77b-480c-4f64-9485-95be805bc357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to lesson: statistics 1 then statistics 3\n",
    "# then go to univariate Non graphical analysis\n",
    "# detect outliers using numerical statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee200da8-62b0-492d-b118-f4d665a1fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check outliers: down in the data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fda0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66d611-6958-4860-8522-9ada7fce40b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to delete ouliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e886ec7-388c-414b-ada7-803c2fb1f2cb",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning: \n",
    "\n",
    "Preliminary findings from data profiling can lead to cleaning the data by:\n",
    "- Handling missing values\n",
    "- Correcting errors.\n",
    "- Dealing with outliers.\n",
    "\n",
    "-------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21876b48-f5ec-4970-85a9-0520d45d8841",
   "metadata": {},
   "source": [
    "### Handling missing values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890d5a2-2a65-4090-9427-f89c0f011d3f",
   "metadata": {},
   "source": [
    "**Why my data has missing values?**\n",
    "They may occur at two stages:\n",
    "1. Data Extraction: It is possible that there are problems with extraction process. Errors at data extraction stage are typically easy to find and can be corrected easily as well.\n",
    "2. Data collection: These errors occur at time of data collection and are harder to correct.\n",
    "\n",
    "**Why do we need to handle the missing data?**\n",
    "To avoid:\n",
    "- Bias the conclusions.\n",
    "- Leading the business to make wrong decisions.\n",
    "\n",
    "**Which are the methods to treat missing values ?**\n",
    "1. Deletion: we delete rows where any of the variable is missing. Simplicity is one of the major advantage of this method, but this method reduces the power of model because it reduces the sample size.\n",
    "\n",
    "2. Imputation: is a method to fill in the missing values with estimated ones. This imputation is one of the most frequently used methods.\n",
    "\n",
    "    2.1. Mean/ Mode/ Median Imputation: It consists of replacing the missing data for a given attribute by the mean or median (quantitative attribute) or mode (qualitative attribute) of all known values of that variable.\n",
    "    > It can be of two types:\n",
    "    > - Generalized Imputation: In this case, we calculate the mean or median for all non missing values of that variable then replace missing value with mean or median.\n",
    "    > - Similar case Imputation: In this case, we calculate average for each group individually of non missing values then replace the missing value based on the group.\n",
    "\n",
    "    2.2. Constant Value\n",
    "   \n",
    "    2.3. Forward Filling\n",
    "   \n",
    "    2.4. Backward Filling\n",
    "\n",
    "6. Prediction Model:  Prediction model is one of the sophisticated method for handling missing data. Here, we create a predictive model to estimate values that will substitute the missing data.  In this case, we divide our data set into two sets: One set with no missing values for the variable and another one with missing values. First data set become training data set of the model while second data set with missing values is test data set and variable with missing values is treated as target variable. Next, we create a model to predict target variable based on other attributes of the training data set and populate missing values of test data set.\n",
    "\n",
    "> There are 2 drawbacks for this approach:\n",
    "> - The model estimated values are usually more well-behaved than the true values\n",
    "> - If there are no relationships with attributes in the data set and the attribute with missing values, then the model will not be precise for estimating missing values.\n",
    "\n",
    "9. KNN Imputation: In this method of imputation, the missing values of an attribute are imputed using the given number of attributes that are most similar to the attribute whose values are missing. The similarity of two attributes is determined using a distance function. It is also known to have certain advantage & disadvantages.\n",
    "\n",
    "   > **Advantages:**\n",
    "   > - k-nearest neighbour can predict both qualitative & quantitative attributes\n",
    "   > - Creation of predictive model for each attribute with missing data is not required\n",
    "   > - Attributes with multiple missing values can be easily treated\n",
    "   > - Correlation structure of the data is taken into consideration\n",
    "\n",
    "   > **Disadvantage:**\n",
    "   > - KNN algorithm is very time-consuming in analyzing large database. It searches through all the dataset looking for the most similar instances.\n",
    "   > - Choice of k-value is very critical. Higher value of k would include attributes which are significantly different from what we need whereas lower value of k implies missing out of significant attributes.\n",
    "\n",
    "--------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop irrelevant columns\n",
    "# dropping job date column\n",
    "df.drop(columns=['job_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3886956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e741fb-71c1-46ad-a526-d8f0b1564dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop dulicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.shape  # we can see that we dropped our duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with nulls\n",
    "\n",
    "# comp_size        14\n",
    "# eco_activity     83\n",
    "# qualif          667\n",
    "\n",
    "df[df['comp_size'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3035c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comp_size'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['eco_activity'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773bcbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['qualif'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to deal with these null values i will fill them with '!مجهول' \n",
    "# because it might be null because the company didn't fill these details that were not mandatory to fill "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e64af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st cleaning column: comp_size\n",
    "\n",
    "df['comp_size'] = df['comp_size'].fillna('!مجهول')\n",
    "df[df['comp_size'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a05a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd cleaning column: eco_activity\n",
    "\n",
    "df['eco_activity'] = df['eco_activity'].fillna('!مجهول')\n",
    "df[df['eco_activity'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02df83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd cleaning column: qualif\n",
    "\n",
    "df['qualif'] = df['qualif'].fillna('!مجهول')\n",
    "df[df['qualif'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01287962-8077-4c01-8d1d-5f8aed6cb37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 'positions' column data type\n",
    "# 1st I will split the values from / and keep only the number\n",
    "\n",
    "#df['positions'] = df['positions'].map(lambda r: r.split('')[0])\n",
    "df['positions'] = df['positions'].map(lambda r: r.split('/')[1] if '/' in r else r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc72e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d147b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing data type of positions to int\n",
    "df['positions'] = df['positions'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['benefits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['benefits'].map(lambda r: r.split('Salary')[1] if 'Salary' in r else r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f760bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# i converted the benifits column to a list so i can extract the second element (the salary number) and change it to a float for further analysis \n",
    "df['benefits'] = df['benefits'].apply(ast.literal_eval)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f942a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted the second index and converting the number to float\n",
    "df['benefits'] = df['benefits'].apply(lambda r: float(r[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09370ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['benefits'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a04c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7171bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cee10f-0af8-44e5-b595-8e965294daad",
   "metadata": {},
   "source": [
    "### Correcting errors\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f1e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckecking outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2dd1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking\n",
    "emp_report = sv.analyze(df)\n",
    "emp_report.show_html('Sweetviz_Report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06273b88-a169-42e8-81f5-5d71cb3f9c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e6faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548eccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d95da5-a3ba-473a-8243-aa177cadae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to 7th dimension Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc7dbb-6867-44cf-8f99-1b969a80be40",
   "metadata": {},
   "source": [
    "### Dealing with outliers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88347887-4bdf-48af-9486-cb8fe80c97af",
   "metadata": {},
   "source": [
    "**How to remove Outliers?**\n",
    "Most of the ways to deal with outliers are similar to the methods of missing values like deleting rows, transforming them, binning them, treat them as a separate group, imputing values and other statistical methods. Here, we will discuss the common techniques used to deal with outliers:\n",
    "\n",
    "1. Deleting rows: We delete outlier values if it is due to data entry error, data processing error or outlier rows are very small in numbers. We can also use trimming at both ends to remove outliers.\n",
    "\n",
    "2. Imputing: Like imputation of missing values, we can also impute outliers. We can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier rows and after that we can impute it with predicted values.\n",
    "\n",
    "3. Treat separately: If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a585f0-2b9c-42fa-bf21-cacc6aa3be3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98548b66-e309-4eb8-a5dc-65fe0abbf1e8",
   "metadata": {},
   "source": [
    "## 3. Univariate Analysis: \n",
    "\n",
    "This involves examining single variables to understand their characteristics (distribution, central tendency, dispersion, and shape).\n",
    "\n",
    "We calculate **numerical values** about the data that tells us about the distribution of the data. We also **draw graphs** showing visually how the data is distributed. **To answer the following questions about Features/characteristics of Data:**\n",
    "- Where is the center of the data? (location)\n",
    "- How much does the data vary? (scale)\n",
    "- What is the shape of the data? (shape)\n",
    "\n",
    "**The benefits of this analysis:**\n",
    "Statistics summary gives a high-level idea to identify whether the data has any outliers, data entry error, distribution of data such as the data is normally distributed or left/right skewed\n",
    "\n",
    "**In this step, we will explore variables one by one using following approaches:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6718111-7ac2-4fff-9851-654655b62e0b",
   "metadata": {},
   "source": [
    "### 1. Univariate Graphical Analysis:\n",
    "Method to perform uni-variate analysis will depend on whether the variable type is categorical or numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db79d803-b75c-4686-a623-e8420321a90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d57c4b9-9bb3-494d-85dd-b8d39afda139",
   "metadata": {},
   "source": [
    "#### I. Categorical Variables:\n",
    "\n",
    "we’ll use frequency table to understand distribution of each category\n",
    "- Bar Chart (Ordinal) - Orderd\n",
    "- Pie Chart (Nominal) - non Orderd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ece4b7-5508-403a-8fb0-f519fc74272a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2641f0d-80f2-493b-99b7-15476379a1e2",
   "metadata": {},
   "source": [
    "#### II. Numerical Variables:\n",
    "\n",
    "we need to understand the central tendency and spread of the variable (Descriptive Analysis) using:\n",
    "   - Box plot\n",
    "   - Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be16f08-a072-4a02-a3ee-6f9d57786fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d66677d3-44ad-414f-9b39-f9c1995ad043",
   "metadata": {},
   "source": [
    "### 2. Univariate Non-Graphical analysis: \n",
    "\n",
    "- Where is the center of the data? (location) --> **Measures of central tendency**\n",
    "- How much does the data vary? (scale) --> **Measure of variability**\n",
    "- What is the shape of the data? (shape) --> **Measures of variation combined with an average (measure of center) gives a good picture of the distribution of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1058d84-e61e-4f5b-b66a-29931e77821b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f09bb809-0edd-432b-bf23-abd0840b3e16",
   "metadata": {},
   "source": [
    "## 4. Bivariate/Multivariate Analysis:\n",
    "\n",
    "Here, you look at the relationships between two or more variables. This can involve looking for correlations, patterns, and trends that suggest a relationship or an association.\n",
    "\n",
    "We can perform bi-variate analysis for any combination of categorical and numerical variables. The combination can be:\n",
    "| bi-variate variables   | Plot type |\n",
    "| ------------- | ------------- |\n",
    "| Categorical & Categorical| Stacked Bar Chart |\n",
    "| Categorical & numerical  | scatter plot, histogram, box plot|\n",
    "| numerical  & numerical  | Scatter plot, line chart| \n",
    "\n",
    "\n",
    "Multivariate Analysis:\n",
    "- Heat map\n",
    "- Bar Chart\n",
    "- Scatter Chart\n",
    "- Line Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f2864-ce9e-4f15-99c8-052d053154a7",
   "metadata": {},
   "source": [
    "**Categorical & Categorical --> (Stacked Column Chart)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9f2ac-8477-49ea-9c68-4fe44d4395b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "730e088f-fe13-40da-8fbb-686f5135fa4d",
   "metadata": {},
   "source": [
    "**Categorical & numerical --> (scatter plot, histogram, box plot)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3590e345-2051-4215-91ac-07d196b510ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20aed48d-b78e-46da-963f-e7f15e2f4dc7",
   "metadata": {},
   "source": [
    "**numerical & numerical --> (Scatter plot, line chart)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41023177-01cb-4f39-a750-12be71b13bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efb43b30-b1f0-48a4-a19c-195810cc8a0f",
   "metadata": {},
   "source": [
    "We could also use a correlation matrix to get more specific information about the relationship between these two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406f58f-b825-4a15-8f6e-c68ca66bd483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91224405",
   "metadata": {},
   "source": [
    "## Q1: What proportion of job postings is attributed to each region within the kingdom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it shows that Riyadh has the maximum number of job postings because it's the capital city of KSA, \n",
    "# after that Makkah comes which made me suprised! i thnk it has max number because now everyone can easily visit saudi and do hajj and umrah and that affects on the market their too\n",
    "df['region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe9b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams['font.family'] = 'Noto Sans Arabic'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65c334",
   "metadata": {},
   "source": [
    "## Q2: Is there a gender preference indicated in the job postings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6afcafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yes as the data show there are many companies that prefer to hire males than females, and this might lead to another question\n",
    "# which jobs are most likely they prefer to hire males? or females?\n",
    "gender_counts = df['gender'].value_counts()\n",
    "gender_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(gender_counts.index, gender_counts.values, color='skyblue')  # Plot horizontal bar chart\n",
    "\n",
    "plt.barh(gender_counts.index, gender_counts.values, color='mediumpurple'\n",
    "         \n",
    ")\n",
    "plt.xlabel('Job Posings')\n",
    "plt.ylabel('Gender Preference')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e36106",
   "metadata": {},
   "source": [
    "## Q3: What is the expected salary range for fresh graduates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's see the fresh grad who have 0 experience because they just got graduated\n",
    "df_fresh_grad = df[df['exper'] == '0 Years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39531d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then let's see the min and max salary of them\n",
    "min_salary = df_fresh_grad['benefits'].min()\n",
    "max_salary = df_fresh_grad['benefits'].max()\n",
    "\n",
    "print('The range salary of fresh graduates is around: ', max_salary , ' and ', min_salary) \n",
    "# which is in reality seems real "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7482e926",
   "metadata": {},
   "source": [
    "## Q4: Are job opportunities predominantly targeted at individuals with experience, or is there room for fresh graduates as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9008898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as it shows here, the number of job posting with 0 experience that most likely targets fresh graduates is 805 posting\n",
    "df_fresh_grad['exper'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4cd83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_exp = df[df['exper'] >= '1 Years'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1179f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_exp['exper'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d14035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and for the job postings with 1 year or more experience is 619, which leads that alway a room for fresh graduates\n",
    "# and i think the reason is because the salary won't be high like the ones with experience \n",
    "df_with_exp['exper'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713968b",
   "metadata": {},
   "source": [
    "## Q5:  Which jobs are most likely they prefer to hire males? or females?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering females job postings\n",
    "female_pref = df[df['gender'] == 'F']['job_title']\n",
    "female_pref.value_counts().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see here that accountant is the most job post that they prefer females for it \n",
    "female_pref.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46894280",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_pref.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering males job postings\n",
    "male_pref = df[df['gender'] == 'M']['job_title']\n",
    "male_pref.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b36507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see here that salesman job is what they prefer for males as a job posting\n",
    "male_pref.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_pref.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638fd0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
